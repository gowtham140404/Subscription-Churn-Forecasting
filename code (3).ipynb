{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Subscription Churn Forecasting (Jan 2025 - May 2025)\n",
        "Author: Dr. Neha Sardana\n",
        "\n",
        "Pipeline:\n",
        " - Load and clean telecom dataset (200K+ customers)\n",
        " - Handle missing values & categorical encoding\n",
        " - Balance data with SMOTE\n",
        " - Train Gradient Boosting model\n",
        " - Evaluate (precision, recall, F1, ROC-AUC)\n",
        " - Export feature importance for retention strategies\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMB_AVAILABLE = False\n",
        "\n",
        "\n",
        "# ---------------- Data Cleaning ----------------\n",
        "def load_and_clean(file, target_col=\"churn\"):\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Drop duplicates\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Handle missing values (example strategy)\n",
        "    for col in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
        "        df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "    # Encode categorical features\n",
        "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
        "        if col != target_col:\n",
        "            df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col].map({\"Yes\": 1, \"No\": 0}) if df[target_col].dtype == \"object\" else df[target_col]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# ---------------- Train/Test Split + SMOTE ----------------\n",
        "def prepare_data(X, y, test_size=0.2, scale=True, balance=True):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size, random_state=42)\n",
        "\n",
        "    if balance and IMB_AVAILABLE:\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# ---------------- Modeling ----------------\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_names):\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\n=== Gradient Boosting Evaluation ===\")\n",
        "    print(classification_report(y_test, preds, digits=4))\n",
        "    print(\"ROC-AUC:\", round(roc_auc_score(y_test, probs), 4))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance\n",
        "    importances = model.feature_importances_\n",
        "    feat_imp = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "    feat_imp = feat_imp.sort_values(\"importance\", ascending=False)\n",
        "    feat_imp.to_csv(\"reports/feature_importances.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=feat_imp.head(15), palette=\"viridis\")\n",
        "    plt.title(\"Top 15 Churn Drivers\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    X, y = load_and_clean(\"data/customers.csv\", target_col=\"churn\")\n",
        "    X_train, X_test, y_train, y_test = prepare_data(X, y)\n",
        "\n",
        "    model = train_and_evaluate(X_train, X_test, y_train, y_test, feature_names=list(X.columns))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9VHOvCkKDnuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}